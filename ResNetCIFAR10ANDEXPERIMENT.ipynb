{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83oUSr7vMvfI"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkZIXcPBMvfo"
   },
   "source": [
    "Next we need to initialize the weights of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgxYggeoMvfs"
   },
   "outputs": [],
   "source": [
    "class CIFAR10Subset(torchvision.datasets.CIFAR10):\n",
    "    \"\"\"\n",
    "    Get a subset of the CIFAR10 dataset, according to the passed indices.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, idx=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        if idx is None:\n",
    "            return\n",
    "        \n",
    "        self.data = self.data[idx]\n",
    "        targets_np = np.array(self.targets)\n",
    "        self.targets = targets_np[idx].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTQRrktlMvfv"
   },
   "source": [
    "We next define transformations that change the images into PyTorch tensors, standardize the values according to the precomputed mean and standard deviation, and provide data augmentation for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rM9bzsLiMvfv"
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, 4),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GgqQ-o2JMvfy",
    "outputId": "a08e2da1-240b-41d1-8fd6-eef66668f048"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "ntrain = 45_000\n",
    "train_set = CIFAR10Subset(root='./data', train=True, idx=range(ntrain), \n",
    "                          download=True, transform=transform_train)\n",
    "val_set = CIFAR10Subset(root='./data', train=True, idx=range(ntrain, 50_000), \n",
    "                        download=True, transform=transform_eval)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=transform_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLzDxt7YMvf2"
   },
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(train_set, batch_size=128,\n",
    "                                                   shuffle=True, num_workers=2,\n",
    "                                                   pin_memory=True)\n",
    "dataloaders['val'] = torch.utils.data.DataLoader(val_set, batch_size=128,\n",
    "                                                 shuffle=False, num_workers=2,\n",
    "                                                 pin_memory=True)\n",
    "dataloaders['test'] = torch.utils.data.DataLoader(test_set, batch_size=128,\n",
    "                                                  shuffle=False, num_workers=2,\n",
    "                                                  pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTscp5KoMvf4"
   },
   "source": [
    "Next we push the model to our GPU (if there is one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v8ayUMcwMvf5",
    "outputId": "fe7ca0cf-ef51-4379-bf36-bfeb2299eabc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "resnet.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYjLjeye_sBJ"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "num_ftrs = resnet.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "resnet.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "resnet = resnet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sIKgMvbMvf7"
   },
   "source": [
    "Next we define a helper method that does one epoch of training or evaluation. We have only defined training here, so you need to implement the necessary changes for evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrNDiLTBMvf8"
   },
   "outputs": [],
   "source": [
    "def run_epoch(model, optimizer, dataloader, train):\n",
    "    \"\"\"\n",
    "    Run one epoch of training or evaluation.\n",
    "    \n",
    "    Args:\n",
    "        model: The model used for prediction\n",
    "        optimizer: Optimization algorithm for the model\n",
    "        dataloader: Dataloader providing the data to run our model on\n",
    "        train: Whether this epoch is used for training or evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Loss and accuracy in this epoch.\n",
    "    \"\"\"\n",
    "    # TODO: Change the necessary parts to work correctly during evaluation (train=False)\n",
    "\n",
    "    # device doesn't change depending on eval() or train()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    if not train: \n",
    "        model.eval()\n",
    "        \n",
    "        # for stats at the end\n",
    "        epoch_loss = 0.0\n",
    "        epoch_acc = 0.0\n",
    "        \n",
    "        # Iterate over data\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            # No optimizer, comp.graph.tracking, backprop is necessary\n",
    "            pred =  model(xb)\n",
    "            loss = F.cross_entropy(pred, yb)\n",
    "            # Make probs\n",
    "            #prob = F.softmax(pred, dim=1)\n",
    "            # Get class with highest prob.\n",
    "            top1 = torch.argmax(pred, dim=1)\n",
    "            # Check how many predictions are correct for examples in current batch\n",
    "            ncorrect = torch.sum(top1 == yb)\n",
    "            \n",
    "            # statistics\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += ncorrect.item()\n",
    "            \n",
    "        epoch_loss /= len(dataloader.dataset)\n",
    "        epoch_acc /= len(dataloader.dataset)\n",
    "                 \n",
    "    else: \n",
    "        # Set model to training mode (for e.g. batch normalization, dropout)\n",
    "        model.train()\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        epoch_acc = 0.0\n",
    "\n",
    "        # Iterate over data\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            with torch.set_grad_enabled(True):\n",
    "                pred = model(xb)\n",
    "                loss = F.cross_entropy(pred, yb)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            #prob = F.softmax(pred, dim=1)\n",
    "            top1 = torch.argmax(pred, dim=1)\n",
    "            ncorrect = torch.sum(top1 == yb)\n",
    "            epoch_loss += loss.item()\n",
    "            # Doesn't make total sense because here batch_norm and potentially dropout are still applied for the outputs\n",
    "            epoch_acc += ncorrect.item()\n",
    "\n",
    "        epoch_loss /= len(dataloader.dataset)\n",
    "        epoch_acc /= len(dataloader.dataset)\n",
    "        \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb-m7nlyMvf_"
   },
   "source": [
    "Next we implement a method for fitting (training) our model. For many models early stopping can save a lot of training time. Your task is to add early stopping to the loop (based on validation accuracy). Early stopping usually means exiting the training loop if the validation accuracy hasn't improved for `patience` number of steps. Don't forget to save the best model parameters according to validation accuracy. You will need `copy.deepcopy` and the `state_dict` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rlhAoSjMvgB"
   },
   "outputs": [],
   "source": [
    "info_train = []\n",
    "info_val = []\n",
    "info_test = []\n",
    "def fit(model, optimizer, lr_scheduler, dataloaders, start_epochs, max_epochs, patience):\n",
    "    \"\"\"\n",
    "    Fit the given model on the dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: The model used for prediction\n",
    "        optimizer: Optimization algorithm for the model\n",
    "        lr_scheduler: Learning rate scheduler that improves training\n",
    "                      in late epochs with learning rate decay\n",
    "        dataloaders: Dataloaders for training and validation\n",
    "        max_epochs: Maximum number of epochs for training\n",
    "        patience: Number of epochs to wait with early stopping the\n",
    "                  training if validation loss has decreased\n",
    "                  \n",
    "    Returns:\n",
    "        Loss and accuracy in this epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    best_acc = 0\n",
    "    curr_patience = 0\n",
    "    \n",
    "    for epoch in range(start_epochs,max_epochs):\n",
    "        train_loss, train_acc = run_epoch(model, optimizer, dataloaders['train'], train=True)\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        epoch += 1\n",
    "        print(f\"Epoch {epoch : >3}/{max_epochs}\")\n",
    "        print(f\"train loss: {train_loss}, accuracy: {train_acc * 100:.2f}%\")\n",
    "        val_loss, val_acc = run_epoch(model, None, dataloaders['val'], train=False)\n",
    "        print(f\"val loss: {val_loss}, accuracy: {val_acc * 100:.2f}%\")\n",
    "        test_loss, test_acc = run_epoch(resnet, None, dataloaders['test'], train=False)\n",
    "        print(f\"Test loss: {test_loss}, accuracy: {test_acc * 100:.2f}%\")\n",
    "        \n",
    "\n",
    "        info_train.append({'epoch':epoch,'loss':train_loss,'acc':train_acc})\n",
    "        info_val.append({'epoch':epoch,'loss':val_loss,'acc':val_acc}) \n",
    "        info_test.append({'epoch':epoch,'loss':test_loss,'acc':test_acc})  \n",
    "          \n",
    "        # TODO: Add early stopping and save the best weights (in best_model_weights)\n",
    "        curr_patience += 1\n",
    "        \n",
    "        if val_acc > best_acc: \n",
    "            # Copy current best model, simple assignment won't work because it only copies a pointer on model which is still trained\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            # Save Model, Where is copy.deepcopy necessary?\n",
    "            #torch.save(model.state_dict(), 'best_model.pt')\n",
    "            # Reset iteration variables \n",
    "            curr_patience = 0 \n",
    "            best_acc = val_acc\n",
    "        \n",
    "        if curr_patience > patience: \n",
    "            break \n",
    "  \n",
    "    # Both ways should work (Together with the above)\n",
    "    #model.load_state_dict(torch.load('best_model.pt'))\n",
    "    # load_state_dict takes in a dict_object NOT a path directly!\n",
    "    model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKUjdvKfMvgE"
   },
   "source": [
    "In most cases you should just use the Adam optimizer for training, because it works well out of the box. However, a well-tuned SGD (with momentum) will in most cases outperform Adam. And since the original paper gives us a well-tuned SGD we will just use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "id": "bQb3P8csMvgF",
    "outputId": "55c59079-a437-41e4-ce05-d37c1a227efd"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-133-eee0ec025fd4>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    fit(resnet, optimizer, lr_scheduler, dataloaders,start_epoch=1 max_epochs=100, patience=50)\u001b[0m\n\u001b[0m                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(resnet.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n",
    "\n",
    "# Fit model\n",
    "fit(resnet, optimizer, lr_scheduler, dataloaders,start_epoch=1 max_epochs=100, patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yVZyCL77LJWK",
    "outputId": "c727528a-4dad-4ba3-8ea8-cf9d369119e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/200\n",
      "train loss: 0.0030724759429693223, accuracy: 86.39%\n",
      "val loss: 0.004363092267513275, accuracy: 82.40%\n",
      "Test loss: 0.004375452882051468, accuracy: 82.39%\n",
      "Epoch 102/200\n",
      "train loss: 0.0030346389796998764, accuracy: 86.44%\n",
      "val loss: 0.0041631247699260715, accuracy: 84.24%\n",
      "Test loss: 0.004352417472004891, accuracy: 82.73%\n",
      "Epoch 103/200\n",
      "train loss: 0.003071366681655248, accuracy: 86.34%\n",
      "val loss: 0.004502340495586395, accuracy: 82.28%\n",
      "Test loss: 0.004484102170169354, accuracy: 81.83%\n",
      "Epoch 104/200\n",
      "train loss: 0.0030369005660216015, accuracy: 86.24%\n",
      "val loss: 0.004570019352436065, accuracy: 82.56%\n",
      "Test loss: 0.004546239498257637, accuracy: 80.95%\n",
      "Epoch 105/200\n",
      "train loss: 0.0030637056701713137, accuracy: 86.23%\n",
      "val loss: 0.0043779697895050046, accuracy: 82.26%\n",
      "Test loss: 0.004305039814114571, accuracy: 82.08%\n",
      "Epoch 106/200\n",
      "train loss: 0.002981569932235612, accuracy: 86.67%\n",
      "val loss: 0.00425837140083313, accuracy: 83.02%\n",
      "Test loss: 0.004500808531045914, accuracy: 81.92%\n",
      "Epoch 107/200\n",
      "train loss: 0.003048813991745313, accuracy: 86.40%\n",
      "val loss: 0.005067250925302505, accuracy: 80.04%\n",
      "Test loss: 0.005360010603070259, accuracy: 79.04%\n",
      "Epoch 108/200\n",
      "train loss: 0.003012016009291013, accuracy: 86.38%\n",
      "val loss: 0.004305301678180695, accuracy: 83.04%\n",
      "Test loss: 0.004522691258788109, accuracy: 82.12%\n",
      "Epoch 109/200\n",
      "train loss: 0.0029729963650306068, accuracy: 86.64%\n",
      "val loss: 0.00456677023768425, accuracy: 82.28%\n",
      "Test loss: 0.0046323753267526625, accuracy: 81.63%\n",
      "Epoch 110/200\n",
      "train loss: 0.003019614244500796, accuracy: 86.41%\n",
      "val loss: 0.004029898154735565, accuracy: 83.36%\n",
      "Test loss: 0.004259285190701485, accuracy: 82.12%\n",
      "Epoch 111/200\n",
      "train loss: 0.0029414177225695714, accuracy: 86.81%\n",
      "val loss: 0.004800172793865204, accuracy: 81.94%\n",
      "Test loss: 0.004778914380073548, accuracy: 81.34%\n",
      "Epoch 112/200\n",
      "train loss: 0.002904938617348671, accuracy: 87.01%\n",
      "val loss: 0.004183677744865418, accuracy: 83.34%\n",
      "Test loss: 0.004316612288355827, accuracy: 82.27%\n",
      "Epoch 113/200\n",
      "train loss: 0.0029577723258071478, accuracy: 86.96%\n",
      "val loss: 0.0046398225724697114, accuracy: 82.58%\n",
      "Test loss: 0.004700036117434502, accuracy: 81.79%\n",
      "Epoch 114/200\n",
      "train loss: 0.00292494760453701, accuracy: 86.80%\n",
      "val loss: 0.004272459656000137, accuracy: 83.38%\n",
      "Test loss: 0.004120832088589668, accuracy: 82.98%\n",
      "Epoch 115/200\n",
      "train loss: 0.002871266718705495, accuracy: 87.24%\n",
      "val loss: 0.004281763255596161, accuracy: 82.90%\n",
      "Test loss: 0.004566587656736374, accuracy: 81.34%\n",
      "Epoch 116/200\n",
      "train loss: 0.002906636428833008, accuracy: 87.12%\n",
      "val loss: 0.004748781234025955, accuracy: 82.00%\n",
      "Test loss: 0.004761075469851494, accuracy: 80.73%\n",
      "Epoch 117/200\n",
      "train loss: 0.002902703422307968, accuracy: 86.84%\n",
      "val loss: 0.004368220859766007, accuracy: 83.00%\n",
      "Test loss: 0.004375384941697121, accuracy: 82.49%\n",
      "Epoch 118/200\n",
      "train loss: 0.002832899515165223, accuracy: 87.36%\n",
      "val loss: 0.004592547911405564, accuracy: 82.20%\n",
      "Test loss: 0.004689725843071937, accuracy: 81.47%\n",
      "Epoch 119/200\n",
      "train loss: 0.002858613568213251, accuracy: 87.20%\n",
      "val loss: 0.005072029137611389, accuracy: 81.76%\n",
      "Test loss: 0.0046736401110887525, accuracy: 81.15%\n",
      "Epoch 120/200\n",
      "train loss: 0.0028594621072212854, accuracy: 87.06%\n",
      "val loss: 0.004438333469629288, accuracy: 83.12%\n",
      "Test loss: 0.004550405609607696, accuracy: 82.16%\n",
      "Epoch 121/200\n",
      "train loss: 0.0028342019448677697, accuracy: 87.42%\n",
      "val loss: 0.0042708971202373505, accuracy: 83.54%\n",
      "Test loss: 0.0045948447972536085, accuracy: 81.65%\n",
      "Epoch 122/200\n",
      "train loss: 0.002794455279575454, accuracy: 87.40%\n",
      "val loss: 0.004208575248718262, accuracy: 83.36%\n",
      "Test loss: 0.0046729930996894835, accuracy: 82.07%\n",
      "Epoch 123/200\n",
      "train loss: 0.0027972604834371143, accuracy: 87.56%\n",
      "val loss: 0.004576412487030029, accuracy: 81.82%\n",
      "Test loss: 0.004839626774191856, accuracy: 80.58%\n",
      "Epoch 124/200\n",
      "train loss: 0.0027674119591712953, accuracy: 87.53%\n",
      "val loss: 0.004504128527641297, accuracy: 82.78%\n",
      "Test loss: 0.0045068834632635115, accuracy: 82.16%\n",
      "Epoch 125/200\n",
      "train loss: 0.002787984019186762, accuracy: 87.49%\n",
      "val loss: 0.004148963457345963, accuracy: 83.40%\n",
      "Test loss: 0.004166411074995995, accuracy: 82.97%\n",
      "Epoch 126/200\n",
      "train loss: 0.002749408135149214, accuracy: 87.67%\n",
      "val loss: 0.004661460262537003, accuracy: 81.98%\n",
      "Test loss: 0.004503260299563408, accuracy: 82.26%\n",
      "Epoch 127/200\n",
      "train loss: 0.0027199059148629505, accuracy: 87.87%\n",
      "val loss: 0.004586572366952896, accuracy: 82.34%\n",
      "Test loss: 0.004737730556726456, accuracy: 81.11%\n",
      "Epoch 128/200\n",
      "train loss: 0.0027182399455044004, accuracy: 87.72%\n",
      "val loss: 0.004372933232784271, accuracy: 83.26%\n",
      "Test loss: 0.004345281282067299, accuracy: 82.61%\n",
      "Epoch 129/200\n",
      "train loss: 0.0027508038000928032, accuracy: 87.76%\n",
      "val loss: 0.00427440505027771, accuracy: 83.68%\n",
      "Test loss: 0.0043140359222888945, accuracy: 82.32%\n",
      "Epoch 130/200\n",
      "train loss: 0.0027427697072426476, accuracy: 87.75%\n",
      "val loss: 0.004711984318494797, accuracy: 82.32%\n",
      "Test loss: 0.004532587361335754, accuracy: 81.74%\n",
      "Epoch 131/200\n",
      "train loss: 0.002713394652141465, accuracy: 87.92%\n",
      "val loss: 0.004456012332439423, accuracy: 82.60%\n",
      "Test loss: 0.004375830757617951, accuracy: 82.66%\n",
      "Epoch 132/200\n",
      "train loss: 0.002664502180284924, accuracy: 87.86%\n",
      "val loss: 0.004371040654182434, accuracy: 83.12%\n",
      "Test loss: 0.004453702768683433, accuracy: 82.16%\n",
      "Epoch 133/200\n",
      "train loss: 0.002675124846233262, accuracy: 88.02%\n",
      "val loss: 0.004710796028375625, accuracy: 81.90%\n",
      "Test loss: 0.004823340207338333, accuracy: 81.30%\n",
      "Epoch 134/200\n",
      "train loss: 0.0026724820047616957, accuracy: 88.08%\n",
      "val loss: 0.004432176095247269, accuracy: 82.28%\n",
      "Test loss: 0.004533694584667683, accuracy: 81.79%\n",
      "Epoch 135/200\n",
      "train loss: 0.0026251173628701104, accuracy: 88.31%\n",
      "val loss: 0.004499415338039398, accuracy: 83.64%\n",
      "Test loss: 0.004367470592260361, accuracy: 82.58%\n",
      "Epoch 136/200\n",
      "train loss: 0.0026774510963095558, accuracy: 87.94%\n",
      "val loss: 0.004737420380115509, accuracy: 82.78%\n",
      "Test loss: 0.004675686764717102, accuracy: 82.30%\n",
      "Epoch 137/200\n",
      "train loss: 0.0026727875434690053, accuracy: 88.08%\n",
      "val loss: 0.004851858097314835, accuracy: 81.80%\n",
      "Test loss: 0.004702664214372635, accuracy: 81.69%\n",
      "Epoch 138/200\n",
      "train loss: 0.002666873018278016, accuracy: 88.13%\n",
      "val loss: 0.004684297281503678, accuracy: 83.26%\n",
      "Test loss: 0.0046387227088212964, accuracy: 81.64%\n",
      "Epoch 139/200\n",
      "train loss: 0.002654551284511884, accuracy: 87.98%\n",
      "val loss: 0.004566856104135513, accuracy: 82.96%\n",
      "Test loss: 0.00453885386288166, accuracy: 81.92%\n",
      "Epoch 140/200\n",
      "train loss: 0.002596023670501179, accuracy: 88.26%\n",
      "val loss: 0.004393199223279953, accuracy: 83.12%\n",
      "Test loss: 0.004567218431830406, accuracy: 82.16%\n",
      "Epoch 141/200\n",
      "train loss: 0.0026104476971758736, accuracy: 88.33%\n",
      "val loss: 0.004816995733976364, accuracy: 81.80%\n",
      "Test loss: 0.004858857277035713, accuracy: 81.02%\n",
      "Epoch 142/200\n",
      "train loss: 0.002541644054982397, accuracy: 88.81%\n",
      "val loss: 0.004534771865606308, accuracy: 82.70%\n",
      "Test loss: 0.00459551048874855, accuracy: 81.74%\n",
      "Epoch 143/200\n",
      "train loss: 0.0025891029324796463, accuracy: 88.44%\n",
      "val loss: 0.004899283075332642, accuracy: 82.30%\n",
      "Test loss: 0.0049231266170740124, accuracy: 81.26%\n",
      "Epoch 144/200\n",
      "train loss: 0.0025942732042736477, accuracy: 88.41%\n",
      "val loss: 0.004133958208560944, accuracy: 84.18%\n",
      "Test loss: 0.004127402326464653, accuracy: 83.36%\n",
      "Epoch 145/200\n",
      "train loss: 0.0026041666746139526, accuracy: 88.08%\n",
      "val loss: 0.00467675746679306, accuracy: 82.10%\n",
      "Test loss: 0.004689078381657601, accuracy: 81.56%\n",
      "Epoch 146/200\n",
      "train loss: 0.0025697062263886133, accuracy: 88.40%\n",
      "val loss: 0.004846071797609329, accuracy: 83.48%\n",
      "Test loss: 0.004509864896535873, accuracy: 82.53%\n",
      "Epoch 147/200\n",
      "train loss: 0.0025787926071219975, accuracy: 88.47%\n",
      "val loss: 0.004273064434528351, accuracy: 84.12%\n",
      "Test loss: 0.004217817205190658, accuracy: 83.09%\n",
      "Epoch 148/200\n",
      "train loss: 0.002550005355146196, accuracy: 88.49%\n",
      "val loss: 0.004195612365007401, accuracy: 83.80%\n",
      "Test loss: 0.004274108624458313, accuracy: 83.03%\n",
      "Epoch 149/200\n",
      "train loss: 0.002521577445665995, accuracy: 88.75%\n",
      "val loss: 0.00455227655172348, accuracy: 82.44%\n",
      "Test loss: 0.004788772469758987, accuracy: 81.31%\n",
      "Epoch 150/200\n",
      "train loss: 0.0025366182837221357, accuracy: 88.52%\n",
      "val loss: 0.004487872993946075, accuracy: 82.74%\n",
      "Test loss: 0.004377474629878998, accuracy: 82.70%\n",
      "Epoch 151/200\n",
      "train loss: 0.0025409195721149444, accuracy: 88.67%\n",
      "val loss: 0.004695411455631256, accuracy: 82.54%\n",
      "Test loss: 0.004596357247233391, accuracy: 82.13%\n",
      "Epoch 152/200\n",
      "train loss: 0.0025369296646780438, accuracy: 88.63%\n",
      "val loss: 0.00426903772354126, accuracy: 84.02%\n",
      "Test loss: 0.004438187132775783, accuracy: 82.73%\n",
      "Epoch 153/200\n",
      "train loss: 0.002499911121858491, accuracy: 88.68%\n",
      "val loss: 0.00419980319738388, accuracy: 84.80%\n",
      "Test loss: 0.004133500236272812, accuracy: 83.51%\n",
      "Epoch 154/200\n",
      "train loss: 0.0025267185274097654, accuracy: 88.65%\n",
      "val loss: 0.004235633188486099, accuracy: 84.16%\n",
      "Test loss: 0.0042651161432266235, accuracy: 83.32%\n",
      "Epoch 155/200\n",
      "train loss: 0.0024258519954151576, accuracy: 89.21%\n",
      "val loss: 0.004059161430597306, accuracy: 84.70%\n",
      "Test loss: 0.004119554662704468, accuracy: 83.91%\n",
      "Epoch 156/200\n",
      "train loss: 0.0024914673411183886, accuracy: 88.84%\n",
      "val loss: 0.004310836464166641, accuracy: 83.06%\n",
      "Test loss: 0.004314766347408295, accuracy: 82.62%\n",
      "Epoch 157/200\n",
      "train loss: 0.002472552845875422, accuracy: 88.79%\n",
      "val loss: 0.004520626997947693, accuracy: 82.46%\n",
      "Test loss: 0.0045641031444072725, accuracy: 81.76%\n",
      "Epoch 158/200\n",
      "train loss: 0.0024568244066503313, accuracy: 89.02%\n",
      "val loss: 0.004458227205276489, accuracy: 83.48%\n",
      "Test loss: 0.004683208382129669, accuracy: 81.72%\n",
      "Epoch 159/200\n",
      "train loss: 0.0024932892749706904, accuracy: 88.80%\n",
      "val loss: 0.004477625149488449, accuracy: 82.66%\n",
      "Test loss: 0.004395897072553634, accuracy: 82.06%\n",
      "Epoch 160/200\n",
      "train loss: 0.002459352100557751, accuracy: 89.10%\n",
      "val loss: 0.004187003189325332, accuracy: 84.20%\n",
      "Test loss: 0.0043084574460983275, accuracy: 82.53%\n",
      "Epoch 161/200\n",
      "train loss: 0.0024061386095152962, accuracy: 89.27%\n",
      "val loss: 0.004419032657146454, accuracy: 84.20%\n",
      "Test loss: 0.004554615741968155, accuracy: 82.38%\n",
      "Epoch 162/200\n",
      "train loss: 0.002408340190847715, accuracy: 89.20%\n",
      "val loss: 0.004120970457792282, accuracy: 84.44%\n",
      "Test loss: 0.0041960664242506026, accuracy: 83.44%\n",
      "Epoch 163/200\n",
      "train loss: 0.002505644185675515, accuracy: 88.79%\n",
      "val loss: 0.003822565996646881, accuracy: 85.38%\n",
      "Test loss: 0.0040898467063903805, accuracy: 83.42%\n",
      "Epoch 164/200\n",
      "train loss: 0.0024161709364917544, accuracy: 88.96%\n",
      "val loss: 0.004246907317638398, accuracy: 84.08%\n",
      "Test loss: 0.0044303860247135165, accuracy: 82.77%\n",
      "Epoch 165/200\n",
      "train loss: 0.0024439223133855396, accuracy: 88.98%\n",
      "val loss: 0.0048760803163051605, accuracy: 82.30%\n",
      "Test loss: 0.004984220954775811, accuracy: 81.33%\n",
      "Epoch 166/200\n",
      "train loss: 0.002443022113045057, accuracy: 89.06%\n",
      "val loss: 0.0048726787090301515, accuracy: 81.64%\n",
      "Test loss: 0.004824791845679283, accuracy: 81.23%\n",
      "Epoch 167/200\n",
      "train loss: 0.0024186245679855348, accuracy: 89.07%\n",
      "val loss: 0.004334822189807892, accuracy: 83.84%\n",
      "Test loss: 0.0046109710633754734, accuracy: 82.33%\n",
      "Epoch 168/200\n",
      "train loss: 0.002433927317129241, accuracy: 89.30%\n",
      "val loss: 0.004815287393331528, accuracy: 82.56%\n",
      "Test loss: 0.005058154436945915, accuracy: 81.34%\n",
      "Epoch 169/200\n",
      "train loss: 0.0023744418425692454, accuracy: 89.26%\n",
      "val loss: 0.004336784118413925, accuracy: 83.98%\n",
      "Test loss: 0.0044261114090681074, accuracy: 83.02%\n",
      "Epoch 170/200\n",
      "train loss: 0.0024515460090504754, accuracy: 89.01%\n",
      "val loss: 0.004609466475248337, accuracy: 82.56%\n",
      "Test loss: 0.004802928431332111, accuracy: 81.10%\n",
      "Epoch 171/200\n",
      "train loss: 0.0023649666567643482, accuracy: 89.49%\n",
      "val loss: 0.004380780780315399, accuracy: 83.30%\n",
      "Test loss: 0.004383447811007499, accuracy: 82.53%\n",
      "Epoch 172/200\n",
      "train loss: 0.002334774887892935, accuracy: 89.61%\n",
      "val loss: 0.004319125932455063, accuracy: 83.62%\n",
      "Test loss: 0.004316764485836029, accuracy: 82.98%\n",
      "Epoch 173/200\n",
      "train loss: 0.0023736924403243596, accuracy: 89.35%\n",
      "val loss: 0.004912262344360352, accuracy: 82.42%\n",
      "Test loss: 0.004906386715173721, accuracy: 81.10%\n",
      "Epoch 174/200\n",
      "train loss: 0.0023708113034566244, accuracy: 89.19%\n",
      "val loss: 0.004145797795057297, accuracy: 83.30%\n",
      "Test loss: 0.00445938820540905, accuracy: 82.41%\n",
      "Epoch 175/200\n",
      "train loss: 0.002395988518661923, accuracy: 89.16%\n",
      "val loss: 0.004160896044969558, accuracy: 84.12%\n",
      "Test loss: 0.004210063299536705, accuracy: 83.35%\n",
      "Epoch 176/200\n",
      "train loss: 0.0023240876999166275, accuracy: 89.69%\n",
      "val loss: 0.004581055742502212, accuracy: 83.02%\n",
      "Test loss: 0.00450698689520359, accuracy: 82.68%\n",
      "Epoch 177/200\n",
      "train loss: 0.002371142507592837, accuracy: 89.28%\n",
      "val loss: 0.004253956311941147, accuracy: 83.78%\n",
      "Test loss: 0.004466570201516151, accuracy: 82.39%\n",
      "Epoch 178/200\n",
      "train loss: 0.0023478143150607745, accuracy: 89.60%\n",
      "val loss: 0.0043237578809261325, accuracy: 83.18%\n",
      "Test loss: 0.0043368701606988904, accuracy: 82.81%\n",
      "Epoch 179/200\n",
      "train loss: 0.002394161381324132, accuracy: 89.18%\n",
      "val loss: 0.004719410598278045, accuracy: 82.94%\n",
      "Test loss: 0.004719192570447922, accuracy: 81.63%\n",
      "Epoch 180/200\n",
      "train loss: 0.0023383547981580097, accuracy: 89.48%\n",
      "val loss: 0.00439910204410553, accuracy: 83.82%\n",
      "Test loss: 0.004284037634730339, accuracy: 83.47%\n",
      "Epoch 181/200\n",
      "train loss: 0.002332735106680128, accuracy: 89.34%\n",
      "val loss: 0.004387986075878143, accuracy: 83.64%\n",
      "Test loss: 0.004512990182638168, accuracy: 82.34%\n",
      "Epoch 182/200\n",
      "train loss: 0.0023078334420919416, accuracy: 89.75%\n",
      "val loss: 0.004419977074861526, accuracy: 83.90%\n",
      "Test loss: 0.004313811025023461, accuracy: 83.38%\n",
      "Epoch 183/200\n",
      "train loss: 0.002339129379722807, accuracy: 89.47%\n",
      "val loss: 0.004489976435899734, accuracy: 83.78%\n",
      "Test loss: 0.004262841299176216, accuracy: 83.25%\n",
      "Epoch 184/200\n",
      "train loss: 0.0022724323170052636, accuracy: 89.78%\n",
      "val loss: 0.0047452447354793545, accuracy: 82.78%\n",
      "Test loss: 0.004581335188448429, accuracy: 81.99%\n",
      "Epoch 185/200\n",
      "train loss: 0.002290144667029381, accuracy: 89.60%\n",
      "val loss: 0.004399364191293716, accuracy: 83.22%\n",
      "Test loss: 0.0043660253882408144, accuracy: 83.01%\n",
      "Epoch 186/200\n",
      "train loss: 0.002287964509593116, accuracy: 89.70%\n",
      "val loss: 0.004178649336099624, accuracy: 83.96%\n",
      "Test loss: 0.004282802301645279, accuracy: 83.09%\n",
      "Epoch 187/200\n",
      "train loss: 0.0022904267970058653, accuracy: 89.62%\n",
      "val loss: 0.00473244309425354, accuracy: 82.98%\n",
      "Test loss: 0.0045471002459526065, accuracy: 82.59%\n",
      "Epoch 188/200\n",
      "train loss: 0.0023310431689023974, accuracy: 89.43%\n",
      "val loss: 0.004391654568910598, accuracy: 83.42%\n",
      "Test loss: 0.004300703048706054, accuracy: 82.79%\n",
      "Epoch 189/200\n",
      "train loss: 0.0022613422287835016, accuracy: 89.76%\n",
      "val loss: 0.004403440713882446, accuracy: 84.02%\n",
      "Test loss: 0.004260793921351433, accuracy: 83.82%\n",
      "Epoch 190/200\n",
      "train loss: 0.0023066370334890152, accuracy: 89.59%\n",
      "val loss: 0.00468490161895752, accuracy: 83.14%\n",
      "Test loss: 0.004728459376096726, accuracy: 81.64%\n",
      "Epoch 191/200\n",
      "train loss: 0.00227740946892235, accuracy: 89.71%\n",
      "val loss: 0.004449575364589691, accuracy: 84.20%\n",
      "Test loss: 0.004411828544735909, accuracy: 82.96%\n",
      "Epoch 192/200\n",
      "train loss: 0.002249268655313386, accuracy: 89.86%\n",
      "val loss: 0.004581257146596908, accuracy: 83.30%\n",
      "Test loss: 0.004333091735839844, accuracy: 83.07%\n",
      "Epoch 193/200\n",
      "train loss: 0.0022306862781445187, accuracy: 89.81%\n",
      "val loss: 0.0053852473199367526, accuracy: 80.42%\n",
      "Test loss: 0.005666595596075058, accuracy: 79.46%\n",
      "Epoch 194/200\n",
      "train loss: 0.002218555327090952, accuracy: 90.02%\n",
      "val loss: 0.004948116636276245, accuracy: 82.84%\n",
      "Test loss: 0.004778649106621742, accuracy: 81.86%\n",
      "Epoch 195/200\n",
      "train loss: 0.0022456994381215836, accuracy: 89.91%\n",
      "val loss: 0.00437641841173172, accuracy: 83.98%\n",
      "Test loss: 0.0043014127813279625, accuracy: 83.37%\n",
      "Epoch 196/200\n",
      "train loss: 0.002250456272231208, accuracy: 89.75%\n",
      "val loss: 0.004678914999961853, accuracy: 82.70%\n",
      "Test loss: 0.004522078970074654, accuracy: 82.28%\n",
      "Epoch 197/200\n",
      "train loss: 0.0022407144985265204, accuracy: 90.00%\n",
      "val loss: 0.0049910295128822325, accuracy: 82.72%\n",
      "Test loss: 0.004777144446969033, accuracy: 81.48%\n",
      "Epoch 198/200\n",
      "train loss: 0.0022105217879017196, accuracy: 90.02%\n",
      "val loss: 0.004581926763057709, accuracy: 83.54%\n",
      "Test loss: 0.004494559545814991, accuracy: 82.76%\n",
      "Epoch 199/200\n",
      "train loss: 0.0021608963125281863, accuracy: 90.39%\n",
      "val loss: 0.004131781554222107, accuracy: 84.46%\n",
      "Test loss: 0.004075999480485916, accuracy: 83.88%\n",
      "Epoch 200/200\n",
      "train loss: 0.0022260928003324403, accuracy: 90.02%\n",
      "val loss: 0.004853649371862411, accuracy: 82.74%\n",
      "Test loss: 0.0049357604086399075, accuracy: 81.32%\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(resnet.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n",
    "\n",
    "# Fit model\n",
    "fit(resnet, optimizer, lr_scheduler, dataloaders,start_epochs=100, max_epochs=200, patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azfnw2TicDmo"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(resnet.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n",
    "\n",
    "# Fit model\n",
    "fit(resnet, optimizer, lr_scheduler, dataloaders,start_epochs=100, max_epochs=200, patience=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eav3gpcYMvgL"
   },
   "source": [
    "Once the model is trained we run it on the test set to obtain our final accuracy.\n",
    "Note that we can only look at the test set once, everything else would lead to overfitting. So you _must_ ignore the test set while developing your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aid92rNmMvgM",
    "outputId": "ab358b93-4f66-46de-9379-1830d52527e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0041, accuracy: 0.8342\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = run_epoch(resnet, None, dataloaders['test'], train=False)\n",
    "print(f\"Test loss: {test_loss:.4f}, accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwxDamDvdDRO"
   },
   "outputs": [],
   "source": [
    "info_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRVajU-jdGN2"
   },
   "outputs": [],
   "source": [
    "info_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UL9niK6jdMMP"
   },
   "outputs": [],
   "source": [
    "info_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t9RB2M2uc35m",
    "outputId": "79235274-6cb3-4f50-ada5-82caa649f998"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_V9x5tOL7rNr"
   },
   "outputs": [],
   "source": [
    "torch.save(resnet,f'drive/MyDrive/ResNetModel/resnetConstantDepth200-0.8342.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GpcZGR2Vd31D",
    "outputId": "980e7538-6282-43b9-93af-02eab48e826b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Params: 23528522\n",
      "23528522\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "# summary(resnet, (3, 224, 224),print_summary=True)\n",
    "# print(count(resnet))\n",
    "# resnet.count_params()\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    # print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "params = count_parameters(model_new)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JIIBW2rcwdQ"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bweHWCm27elN"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "model_new = copy.deepcopy(resnet)\n",
    "def test_remove_layer():\n",
    "    model_new = copy.deepcopy(resnet)\n",
    "    \n",
    "    print(\"Without Removed Layer\")\n",
    "    test_loss, test_acc = run_epoch(resnet, None, dataloaders['test'], train=False)\n",
    "    params = count_parameters(model_new)\n",
    "    print(f\"Test loss: {test_loss:.4f}, accuracy: {test_acc}, params: {params}\")\n",
    "\n",
    "\n",
    "    print(\"CONV_4_2 Removed\")\n",
    "    del model_new.layer4[2]\n",
    "    test_loss, test_acc = run_epoch(model_new, None, dataloaders['test'], train=False)\n",
    "    params = count_parameters(model_new)\n",
    "    print(f\"Test loss: {test_loss:.4f}, accuracy: {test_acc}, params: {params}\")\n",
    "\n",
    "\n",
    "    print(\"CONV_4_1 Removed\")\n",
    "    del model_new.layer4[1]\n",
    "    test_loss, test_acc = run_epoch(model_new, None, dataloaders['test'], train=False)\n",
    "    params = count_parameters(model_new)\n",
    "    print(f\"Test loss: {test_loss:.4f}, accuracy: {test_acc}, params: {params}\")\n",
    "\n",
    "    \n",
    "    print(\"CONV_3_5 Removed\")\n",
    "    del model_new.layer3[5]\n",
    "    test_loss, test_acc = run_epoch(model_new, None, dataloaders['test'], train=False)\n",
    "    params = count_parameters(model_new)\n",
    "    print(f\"Test loss: {test_loss:.4f}, accuracy: {test_acc}, params: {params}\")\n",
    "\n",
    "    \n",
    "    print(\"CONV_3_4 Removed\")\n",
    "    del model_new.layer3[4]\n",
    "    test_loss, test_acc = run_epoch(model_new, None, dataloaders['test'], train=False)\n",
    "    params = count_parameters(model_new)\n",
    "    print(f\"Test loss: {test_loss:.4f}, accuracy: {test_acc}, params: {params}\")\n",
    "\n",
    "\n",
    "    print(\"CONV_3_3 Removed\")\n",
    "    del model_new.layer3[3]\n",
    "    test_loss, test_acc = run_epoch(model_new, None, dataloaders['test'], train=False)\n",
    "    params = count_parameters(model_new)\n",
    "    print(f\"Test loss: {test_loss:.4f}, accuracy: {test_acc}, params: {params}\")\n",
    "\n",
    "    print(\"CONV_3_2 Removed\")\n",
    "    del model_new.layer3[2]\n",
    "    test_loss, test_acc = run_epoch(model_new, None, dataloaders['test'], train=False)\n",
    "    params = count_parameters(model_new)\n",
    "    print(f\"Test loss: {test_loss:.4f}, accuracy: {test_acc}, params: {params}\")\n",
    "\n",
    "\n",
    "    print(\"CONV_3_1 Removed\")\n",
    "    del model_new.layer3[1]\n",
    "    test_loss, test_acc = run_epoch(model_new, None, dataloaders['test'], train=False)\n",
    "    params = count_parameters(model_new)\n",
    "    print(f\"Test loss: {test_loss:.4f}, accuracy: {test_acc}, params: {params}\")\n",
    "\n",
    "    print(\"CONV_2_3 Removed\")\n",
    "    del model_new.layer2[3]\n",
    "    test_loss, test_acc = run_epoch(model_new, None, dataloaders['test'], train=False)\n",
    "    params = count_parameters(model_new)\n",
    "    print(f\"Test loss: {test_loss:.4f}, accuracy: {test_acc}, params: {params}\")\n",
    "\n",
    "    print(\"CONV_2_2 Removed\")\n",
    "    del model_new.layer2[2]\n",
    "    test_loss, test_acc = run_epoch(model_new, None, dataloaders['test'], train=False)\n",
    "    params = count_parameters(model_new)\n",
    "    print(f\"Test loss: {test_loss:.4f}, accuracy: {test_acc}, params: {params}\")\n",
    "\n",
    "    \n",
    "    print(\"CONV_2_1 Removed\")\n",
    "    del model_new.layer2[1]\n",
    "    test_loss, test_acc = run_epoch(model_new, None, dataloaders['test'], train=False)\n",
    "    params = count_parameters(model_new)\n",
    "    print(f\"Test loss: {test_loss:.4f}, accuracy: {test_acc}, params: {params}\")\n",
    "   \n",
    "    # return result_1,result_2,result_3,result_4,result_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sYtb-v_G8kaL",
    "outputId": "c56a7104-5cbd-4441-ef6a-9e4a5ac8f7f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Removed Layer\n",
      "Total Trainable Params: 23528522\n",
      "Test loss: 0.0041, accuracy: 0.8342, params: 23528522\n",
      "CONV_4_2 Removed\n",
      "Total Trainable Params: 19065930\n",
      "Test loss: 0.0045, accuracy: 0.8347, params: 19065930\n",
      "CONV_4_1 Removed\n",
      "Total Trainable Params: 14603338\n",
      "Test loss: 0.0050, accuracy: 0.8355, params: 14603338\n",
      "CONV_3_5 Removed\n",
      "Total Trainable Params: 13486154\n",
      "Test loss: 0.0051, accuracy: 0.8277, params: 13486154\n",
      "CONV_3_4 Removed\n",
      "Total Trainable Params: 12368970\n",
      "Test loss: 0.0051, accuracy: 0.825, params: 12368970\n",
      "CONV_3_3 Removed\n",
      "Total Trainable Params: 11251786\n",
      "Test loss: 0.0052, accuracy: 0.814, params: 11251786\n",
      "CONV_3_2 Removed\n",
      "Total Trainable Params: 10134602\n",
      "Test loss: 0.0054, accuracy: 0.8015, params: 10134602\n",
      "CONV_3_1 Removed\n",
      "Total Trainable Params: 9017418\n",
      "Test loss: 0.0057, accuracy: 0.7895, params: 9017418\n",
      "CONV_2_3 Removed\n",
      "Total Trainable Params: 8737354\n",
      "Test loss: 0.0068, accuracy: 0.7519, params: 8737354\n",
      "CONV_2_2 Removed\n",
      "Total Trainable Params: 8457290\n",
      "Test loss: 0.0076, accuracy: 0.7246, params: 8457290\n",
      "CONV_2_1 Removed\n",
      "Total Trainable Params: 8177226\n",
      "Test loss: 0.0082, accuracy: 0.6977, params: 8177226\n"
     ]
    }
   ],
   "source": [
    "test_remove_layer()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "experimentTesis2ReproduceResnet.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
